{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Sample Baye Opt\n",
    "While a number of production ready algorithms exist such as:\n",
    "- MOE\n",
    "- Spearmint\n",
    "- hyperopt\n",
    "- GPyOpt\n",
    "\n",
    "This notebook handles making a simple algorithm.\n",
    "\n",
    "\n",
    "\n",
    "Simply put, the Bayesian Optimizers requires:\n",
    "- **A Gaussian process** \n",
    "    - ```sklearn.gaussian_process as gp```\n",
    "    - From which we will predict the posterior distribution of the target function (function of errors for models).\n",
    "    - Because a Gaussian distribution is defined by its mean and variance, this model relies on the assumption that the Gaussian process is also completely defined by its mean function and variance function.\n",
    "        - Until math has another revolution and we discover that we know nothing about math (which I seem to find a lot) we can assume this is a pretty safe assumption (GP ~ mean function & variance function).\n",
    "        \n",
    "        \n",
    "    - I can't phrase this better, so:\n",
    "    \n",
    "    ```A GP is a popular probability model, because it induces a posterior distribution over the loss function that is analytically tractable. This allows us to update our beliefs of what looks like, after we have computed the loss for a new set of hyperparameters.``` https://thuijskens.github.io/2016/12/29/bayesian-optimisation/\n",
    "    \n",
    "    \n",
    "    \n",
    "- **An Aquisition function**\n",
    "    - Which decides at which point in our target function, we want to sample next.\n",
    "    \n",
    "    - A number of well documented aquistion functions exist, listed below:\n",
    "        - **Probability of Improvement**\n",
    "            - Looks where a function's **improvement is most likely**\n",
    "            - Can lead to odd behavior because it relies on the current minimum, rather than the magnatude of possiblity of improvement.\n",
    "         \n",
    "        - **Expected improvement (EI)**\n",
    "            - Looks where a function **may most improve**, aka *maximal expected utility* \n",
    "            - EI(x)=ùîº[max{0,f(x)‚àíf(xÃÇ )}]\n",
    "            - Crowd favorite\n",
    "\n",
    "        - **Entropy search**\n",
    "            - Improves function by **minimizing the uncertainty** of any predicted optimium.\n",
    "            \n",
    "        - **Upper Confidence Bound** (UCB)\n",
    "            - Looks where a function's **improvement is most likely**\n",
    "            - Looks to exploits possibly uncertainty by finding where the upperbound may be undetermined.\n",
    "            \n",
    "            \n",
    "As to follow the crowd this notebook will use the Expected Improvement function, for reasons I may revisit this notebook to explain.\n",
    "\n",
    "\n",
    "With these two parts our program should:\n",
    "    * Given observed values of target function f(x), update the posterior expectation of f using the Gaussian Process.\n",
    "    * Find new_x that maximises the ```EI: new_x = argmax EI(x)```.\n",
    "    * Compute the value of f(new_x).\n",
    "    * Update posterior expectation of f\n",
    "    \n",
    "    * Repeat this process for n_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_optimisation(n_iters, sample_loss, bounds,\n",
    "                          n_inital=5,\n",
    "                          alpha=1e-5, epsilon=1e-7):\n",
    "\n",
    "    \"\"\" baye_opt\n",
    "    Uses a Gaussian Processe to optimise the loss function `sample_loss`.\n",
    "    Arguments:\n",
    "    ----------\n",
    "        n_iters: integer.\n",
    "            Number of iterations to run the search algorithm.\n",
    "        \n",
    "        sample_loss: function.\n",
    "            Function to be optimised.\n",
    "        \n",
    "        bounds: array-like, shape = [n_params, 2].\n",
    "            Lower and upper bounds on the parameters of the function `sample_loss`.\n",
    "        \n",
    "        n_inital: integer.\n",
    "            initial number of points from the loss function.\n",
    "            \n",
    "        alpha: float.\n",
    "            Variance of the error term of the GP.\n",
    "        \n",
    "        epsilon: float.\n",
    "            Precision tolerance for floats.\n",
    "    \"\"\"\n",
    "\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "\n",
    "    n_params = bounds.shape[0]\n",
    "\n",
    "    for params in np.random.uniform(bounds[:, 0], bounds[:, 1], (n_inital, bounds.shape[0])):\n",
    "        x_list.append(params)\n",
    "        y_list.append(sample_loss(params))\n",
    "\n",
    "\n",
    "    xp = np.array(x_list)\n",
    "    yp = np.array(y_list)\n",
    "\n",
    "    # Create the GP\n",
    "    kernel = gp.kernels.Matern()\n",
    "    model  = gp.GaussianProcessRegressor(kernel=kernel,\n",
    "                                        alpha=alpha,\n",
    "                                        n_restarts_optimizer=10,\n",
    "                                        normalize_y=True)\n",
    "\n",
    "    for n in range(n_iters):\n",
    "\n",
    "        model.fit(xp, yp)\n",
    "\n",
    "        # Finds the next hyperparameter\n",
    "        next_sample = next_param(expected_improvement, \n",
    "                                 model, yp, greater_is_better=True, \n",
    "                                 bounds=bounds, n_restarts=100)\n",
    "\n",
    "        # Duplicates will break the GP. In case of a duplicate, we will randomly sample a next query point.\n",
    "        if np.any(np.abs(next_sample - xp) <= epsilon):\n",
    "            next_sample = np.random.uniform(bounds[:, 0], bounds[:, 1], bounds.shape[0])\n",
    "            # I did not make this line of code, but damn did it help...\n",
    "            \n",
    "            \n",
    "        # Sample loss for new set of parameters\n",
    "        cv_score = sample_loss(next_sample)\n",
    "        \n",
    "        # Update lists\n",
    "        x_list.append(next_sample)\n",
    "        y_list.append(cv_score)\n",
    "\n",
    "        \n",
    "        # Update xp and yp\n",
    "        xp = np.array(x_list)\n",
    "        yp = np.array(y_list)\n",
    "\n",
    "    return xp, yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_param(acquisition_func, gaussian_process, evaluated_loss, greater_is_better=False,\n",
    "                               bounds=(0, 10), n_restarts=25):\n",
    "    \"\"\" next_param\n",
    "    Proposes the next hyperparameter to sample the loss function for.\n",
    "    Arguments:\n",
    "    ----------\n",
    "        acquisition_func: function.\n",
    "            Acquisition function to optimise.\n",
    "        gaussian_process: GaussianProcessRegressor object.\n",
    "            Gaussian process trained on previously evaluated hyperparameters.\n",
    "        evaluated_loss: array-like, shape = [n_obs,]\n",
    "            Numpy array that contains the values off the loss function for the previously\n",
    "            evaluated hyperparameters.\n",
    "        greater_is_better: Boolean.\n",
    "            Boolean flag that indicates whether the loss function is to be maximised or minimised.\n",
    "        bounds: Tuple.\n",
    "            Bounds for the L-BFGS optimiser.\n",
    "        n_restarts: integer.\n",
    "            Number of times to run the minimiser with different starting points.\n",
    "    \"\"\"\n",
    "    best_x = None\n",
    "    best_acquisition_value = 1\n",
    "    n_params = bounds.shape[0]\n",
    "\n",
    "    for starting_point in np.random.uniform(bounds[:, 0], bounds[:, 1], size=(n_restarts, n_params)):\n",
    "\n",
    "        res = minimize(fun=acquisition_func,\n",
    "                       x0=starting_point.reshape(1, -1),\n",
    "                       bounds=bounds,\n",
    "                       method='L-BFGS-B',\n",
    "                       args=(gaussian_process, evaluated_loss, greater_is_better, n_params))\n",
    "\n",
    "        if res.fun < best_acquisition_value:\n",
    "            best_acquisition_value = res.fun\n",
    "            best_x = res.x\n",
    "\n",
    "    return best_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_improvement(x, gaussian_process, evaluated_loss, greater_is_better=False, n_params=1):\n",
    "    \"\"\" expected_improvement\n",
    "    Expected improvement acquisition function.\n",
    "    Arguments:\n",
    "    ----------\n",
    "        x: array-like, shape = [n_samples, n_hyperparams]\n",
    "            The point for which the expected improvement needs to be computed.\n",
    "        gaussian_process: GaussianProcessRegressor object.\n",
    "            Gaussian process trained on previously evaluated hyperparameters.\n",
    "        evaluated_loss: Numpy array.\n",
    "            Numpy array that contains the values off the loss function for the previously\n",
    "            evaluated hyperparameters.\n",
    "        greater_is_better: Boolean.\n",
    "            Boolean flag that indicates whether the loss function is to be maximised or minimised.\n",
    "        n_params: int.\n",
    "            Dimension of the hyperparameter space.\n",
    "    \"\"\"\n",
    "\n",
    "    x_to_predict = x.reshape(-1, n_params)\n",
    "\n",
    "    mu, sigma = gaussian_process.predict(x_to_predict, return_std=True)\n",
    "\n",
    "    if greater_is_better:\n",
    "        loss_optimum = np.max(evaluated_loss)\n",
    "    else:\n",
    "        loss_optimum = np.min(evaluated_loss)\n",
    "\n",
    "    scaling_factor = (-1) ** (not greater_is_better)\n",
    "\n",
    "    # In case sigma equals zero\n",
    "    with np.errstate(divide='ignore'):\n",
    "        Z = scaling_factor * (mu - loss_optimum) / sigma\n",
    "        expected_improvement = scaling_factor * (mu - loss_optimum) * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        expected_improvement[sigma == 0.0] == 0.0\n",
    "\n",
    "    return -1 * expected_improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "data, target = make_classification(n_samples=2500,\n",
    "                                   n_features=45,\n",
    "                                   n_informative=15,\n",
    "                                   n_redundant=5)\n",
    "\n",
    "def sample_loss(params):\n",
    "    return cross_val_score(SVC(C=10 ** params[0], gamma=10 ** params[1], random_state=12345),\n",
    "                           X=data, y=target, scoring='roc_auc', cv=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "lambdas = np.linspace(1, -4, 25)\n",
    "gammas = np.linspace(1, -4, 20)\n",
    "\n",
    "# We need the cartesian combination of these two vectors\n",
    "param_grid = np.array([[C, gamma] for gamma in gammas for C in lambdas])\n",
    "\n",
    "real_loss = [sample_loss(params) for params in param_grid]\n",
    "\n",
    "# The maximum is at:\n",
    "param_grid[np.array(real_loss).argmax(), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.gaussian_process as gp\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = np.array([[-4, 1], [-4, 1]])\n",
    "\n",
    "xp, yp = bayesian_optimisation(n_iters=10, \n",
    "                               sample_loss=sample_loss, \n",
    "                               bounds=bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp, yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.gaussian_process as gp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eg_function(x): return 1 - (np.exp(-(x - 2)**2) + np.exp(-(x - 6)**2/10) + 1/(x**2 + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2, 8, 10000).reshape(-1, 1)\n",
    "y = eg_function(x)\n",
    "plt.title('eg_function')\n",
    "plt.plot(x, y, c='b');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = [-2,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_x = [[np.random.randint(bounds[0],bounds[1])] for i in range(10)]\n",
    "eg_y = [eg_function(i[0]) for i in eg_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = gp.kernels.Matern()\n",
    "model  = gp.GaussianProcessRegressor(kernel=kernel)\n",
    "\n",
    "model.fit(eg_x,eg_y)\n",
    "\n",
    "model.predict([[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
